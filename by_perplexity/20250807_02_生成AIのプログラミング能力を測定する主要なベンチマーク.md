生成AIのプログラミング能力（コード生成力）を測定する主要なベンチマークは、専門家や比較サイトで広く使われている次の3つが代表的です。

- **HumanEval**  
  OpenAIが公開したプログラミング問題セット。AIが生成したコードが、指定されたテストケースをどの程度正しくパスできるかをスコア（%）で評価します。主にPythonで実装力・ロジックの正確性を測る指標として業界標準になっています。

- **SWE-bench**  
  GitHub上の実際のイシュー（バグや機能追加）の自動修正課題を用いて、AIモデルが本当に実用レベルのコードを自律生成・修正できるかを評価します。難度が高く、より現場に近い「実践力」を測る信頼性の高いベンチマークです。

- **MultiPL-E**  
  複数のプログラミング言語（Python、Java、C++など）で同じ課題を解かせることで、各AIモデルの“多言語対応力”や汎用的なコーディング能力を測定します。幅広い開発現場での適応力を可視化します。

これらベンチマークは、最新の主要比較サイトや開発現場の選定基準でも根拠として頻繁に使われており、特にHumanEvalとSWE-benchは2025年時点で国際的に最も注目されています。[5][6]

加えて、用途特化型の新ベンチマーク（Web開発専用LiveBenchや、ゲーム・データサイエンス分野特化指標など）も登場しており、対象モデル・用途によって使い分けられています。[4][5]

ベンチマークごとに難易度と重視する能力が異なり、総合的な「プログラミング能力」を測るには複数の指標を参考にするのが一般的です。

[1] https://note.com/maruking777/n/nc7599ceab74e
[2] https://bizfreak.co.jp/blog/2ewrh9o1up4
[3] https://make-a-hit.co.jp/column/ai2025/
[4] https://qiita.com/GeneLab_999/items/b54a1bb914877e3c7124
[5] https://ai-kenkyujo.com/news/code-seiseiai/
[6] https://arpable.com/artificial-intelligence/ai-development-support-tools-ranking-2025/
[7] https://note.com/naokun_gadget/n/nf508cfc416d0
[8] https://chatgpt-enterprise.jp/blog/ai-models-2025/
[9] https://miralab.co.jp/media/generative_ai_tools_comparison/
[10] https://dxnavi.com/ai-serivice-2025/